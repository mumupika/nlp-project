{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"opencompass[full]\"\n",
    "%pip install pytorch transformers datasets \"opencompass[full]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装opencompass：Kaggle上已经为我们准备好了其他常用包，只需安装opencompass用于评测即可。如果不在Kaggle上运行，则还需要安装其他必要包。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 指令微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The main program for finetuning LLMs with Huggingface Transformers Library.\n",
    "\n",
    "ALL SECTIONS WHERE CODE POSSIBLY NEEDS TO BE FILLED IN ARE MARKED AS TODO.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Dict\n",
    "import sys\n",
    "import torch\n",
    "from transformers import TrainingArguments, HfArgumentParser, Trainer, AutoTokenizer, AutoModelForCausalLM\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the arguments required for the main program.\n",
    "# NOTE: You can customize any arguments you need to pass in.\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"Arguments for model\n",
    "    \"\"\"\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The path to the LLM to fine-tune or its name on the Hugging Face Hub.\"\n",
    "        }\n",
    "    )\n",
    "    torch_dtype: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Override the default `torch.dtype` and load the model under this dtype.\"\n",
    "            ),\n",
    "            \"choices\": [\"bfloat16\", \"float16\", \"float32\"],\n",
    "        },\n",
    "    )\n",
    "    # TODO: add your model arguments here\n",
    "    pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    \"\"\"Arguments for data\n",
    "    \"\"\"\n",
    "    dataset_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The path to the fine-tuning dataset or its name on the Hugging Face Hub.\"\n",
    "        }\n",
    "    )\n",
    "    # TODO: add your data arguments here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Give three tips for staying healthy.',\n",
       " 'output': '1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.',\n",
       " 'input': None}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Attempt & Debug #####\n",
    "dataset = datasets.load_dataset('./Dataset/alpaca-language-instruction-training')\n",
    "instructions = dataset['train']['instruction']\n",
    "inputs = dataset['train']['input']\n",
    "outputs = dataset['train']['output']\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90cee0dff4eb424cada7e0c4c08fe033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/51760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'output', 'input', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 51760\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Dataset tokenization test\n",
    "dataset = datasets.load_dataset('./Dataset/alpaca-language-instruction-training')\n",
    "tokenizer = AutoTokenizer.from_pretrained('./model/input/qwen2.5/transformers/0.5b/1')\n",
    "model = AutoModelForCausalLM.from_pretrained('./model/input/qwen2.5/transformers/0.5b/1',device_map = 'auto')\n",
    "def tokenize_function(batch):\n",
    "    for i in range(len(batch['input'])):\n",
    "        if batch['input'][i] == None:\n",
    "            batch['input'][i]=''\n",
    "    return tokenizer(batch['instruction'],batch['input'],batch['output'],padding=True, padding_side='left')\n",
    "tokenized_datasets = dataset['train'].map(tokenize_function, batched=True)\n",
    "tokenized_datasets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 51760\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_datasets = tokenized_datasets.remove_columns(['instruction', 'input', 'output'])\n",
    "input_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "tokenizer = AutoTokenizer.from_pretrained('./model/input/qwen2.5/transformers/0.5b/1')\n",
    "model = AutoModelForCausalLM.from_pretrained('./model/input/qwen2.5/transformers/0.5b/1',device_map = 'auto')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Instructions: Tell me the climate in Beijing, the captial of China. The climate in Beijing, the capital of China, is characterized by a temperate continental climate. The area south of the Huai River experiences a subtropical monsoon climate with warm and humid summers, while north of the Huai River and the Inner Mongolia Autonomous Region, it is a subtropical monsoon climate with warm and humid summers. The climate is subhumid, with many rainy days during the dry season. The weather in Beijing is generally cloudy with low temperatures in the summer and high temperatures in the winter. The average temperature range in Beijing is 15.6 to 32.9°F (6.1 to 1.5°C). In the summer, the temperatures can reach as high as 92.2°F (33.8°C) during the day and as low as 80.8°F (27.1°C) at night. In winter, the temperatures can drop to -17.5°F (1.3°C) during the day and -21.3°F (2.6°C) at night. The city is also known for its cold winters and hot summers, making it a prime location for outdoor activities, such as skiing and snowmobiling. The average annual rainfall is 30.1 in (772 mm) with 25.7 inches (647 mm) of snowfall during the winter months.\\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.', \"Instructions: Describe president Blinken of the US. Write the best response;\\nContext: The head of the US Air Force, Gen. Daryl R. Bolger, is expected to be sworn in as the new president on January 20. 2019. Gen. Daryl R. Bolger is the 17th and current chair of the Joint Chiefs of Staff of the US Air Force and the most senior officer to hold the position. Prior to his appointment as the chair, Gen. Bolger was the head of the Air Force's headquarters, the Joint Forces Command, and an officer in the US Army from 1967 to 1982. He currently serves as a senior advisor to President Barack Obama and was previously the deputy director for defense planning for the Department of Defense. The appointment of Gen. Bolger as the new secretary of the Air Force was made by President Barack Obama and is effective immediately.\\n\\nQuestion: what is gen daryl r bolger known for? The answer is Gen. Daryl R. Bolger. He is the former chair of the Joint Chiefs of Staff and the most senior officer in the Air Force. He has a long history in the military and played a key role in Obama's presidency. He was also the deputy director for Defense Planning at the Department of Defense.\\nYou are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.\"]\n"
     ]
    }
   ],
   "source": [
    "### test for inference.\n",
    "\n",
    "input_seqs = [\"Instructions: Tell me the climate in Beijing, the captial of China.\",\n",
    "              \"Instructions: Describe president Blinken of the US.\"]\n",
    "inputs = tokenizer(input_seqs, padding=True,padding_side='left',return_tensors=\"pt\").to(device)\n",
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=2048,\n",
    "    top_p=0.95,\n",
    "    temperature=0.9,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.batch_decode(\n",
    "    generated_ids,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main function\n",
    "# NOTE You can customize some logs to monitor your program.\n",
    "def finetune():\n",
    "    # TODO Step 1: Define an arguments parser and parse the arguments\n",
    "    # NOTE Three parts: model arguments, data arguments, and training arguments\n",
    "    # HINT: Refer to \n",
    "    #   * https://huggingface.co/docs/transformers/v4.46.3/en/internal/trainer_utils#transformers.HfArgumentParser\n",
    "    #   * https://huggingface.co/docs/transformers/v4.46.3/en/main_classes/trainer#transformers.TrainingArguments\n",
    "    parser = HfArgumentParser((ModelArguments, DataArguments,TrainingArguments))\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses(args=sys.argv)\n",
    "\n",
    "    # TODO Step 2: Load tokenizer and model\n",
    "    # HINT 1: Refer to\n",
    "    #   * https://huggingface.co/docs/transformers/v4.46.3/en/main_classes/tokenizer#tokenizer\n",
    "    #   * https://huggingface.co/docs/transformers/v4.46.3/en/model_doc/qwen2\n",
    "    # HINT 2: To save training GPU memory, you need to set the model's parameter precision to half-precision (float16 or bfloat16).\n",
    "    #         You may also check other strategies to save the memory!\n",
    "    #   * https://huggingface.co/docs/transformers/v4.46.3/en/model_doc/llama2#usage-tips\n",
    "    #   * https://huggingface.co/docs/transformers/perf_train_gpu_one\n",
    "    #   * https://www.53ai.com/news/qianyanjishu/2024052494875.html\n",
    "    tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/qwen2.5/transformers/0.5b/1')\n",
    "    model = AutoModelForCausalLM.from_pretrained('/kaggle/input/qwen2.5/transformers/0.5b/1',device_map = 'auto')\n",
    "\n",
    "    # TODO Step 3: Load dataset\n",
    "    # HINT: https://huggingface.co/docs/datasets/v3.1.0/en/package_reference/main_classes#datasets.Dataset\n",
    "    dataset = datasets.load_dataset(data_args.dataset_path)\n",
    "\n",
    "    # TODO Step 4: Define the data collator function\n",
    "    # NOTE During training, for each model parameter update, we fetch a batch of data, perform a forward and backward pass,\n",
    "    # and then update the model parameters. The role of the data collator is to process the data (e.g., padding the data within\n",
    "    # a batch to the same length) and format the batch into the input required by the model.\n",
    "    #\n",
    "    # In this assignment, the purpose of the custom data_collator is to process each batch of data from the dataset loaded in\n",
    "    # Step 3 into the format required by the model. This includes tasks such as tokenizing the data, converting each token into \n",
    "    # an ID sequence, applying padding, and preparing labels.\n",
    "    # \n",
    "    # HINT:\n",
    "    #   * Before implementation, you should:\n",
    "    #      1. Clearly understand the format of each sample in the dataset loaded in Step 3.\n",
    "    #      2. Understand the input format required by the model (https://huggingface.co/docs/transformers/model_doc/qwen2#transformers.Qwen2ForCausalLM).\n",
    "    #         Reading its source code also helps!\n",
    "\n",
    "    def data_collator(batch: List[Dict]):\n",
    "        \"\"\"\n",
    "        batch: list of dict, each dict of the list is a sample in the dataset.\n",
    "        \"\"\"\n",
    "        # The List is a Dict which has the following structure:\n",
    "        # DatasetDict({\n",
    "        #     train: Dataset({\n",
    "        #         features: ['instruction', 'output', 'input'],\n",
    "        #         num_rows: 51760\n",
    "        #     })\n",
    "        # })\n",
    "        # So the trainer may pass the dataset by batch. But what is inside?\n",
    "        \n",
    "        pass\n",
    "\n",
    "    # TODO Step 5: Define the Trainer\n",
    "    # HINT: https://huggingface.co/docs/transformers/main_classes/trainer\n",
    "    trainer = Trainer(\n",
    "        ...,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    # Step 6: Train!\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass your training arguments.\n",
    "# NOTE [IMPORTANT!!!] DO NOT FORGET TO PASS PROPER ARGUMENTS TO SAVE YOUR CHECKPOINTS!!!\n",
    "sys.argv = [\n",
    "    \"notebook\", \n",
    "    \"--arg1\", \"value1\",\n",
    "    \"--arg2\", \"value2\",\n",
    "    ...\n",
    "]\n",
    "finetune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass your training arguments.\n",
    "# NOTE [IMPORTANT!!!] DO NOT FORGET TO PASS PROPER ARGUMENTS TO SAVE YOUR CHECKPOINTS!!!\n",
    "sys.argv = [\n",
    "    \"notebook\", \n",
    "    \"--arg1\", \"value1\",\n",
    "    \"--arg2\", \"value2\",\n",
    "    ...\n",
    "]\n",
    "finetune()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评测模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你有多个GPU，可以修改下面的--hf-num-gpus参数来加速评测."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%opencompass \\\n",
    "    --datasets mmlu_ppl hellaswag_clean_ppl winogrande_ll ARC_e_ppl ARC_c_clean_ppl SuperGLUE_BoolQ_few_shot_ppl \\\n",
    "    --summarizer example \\\n",
    "    --hf-type base \\\n",
    "    --hf-path {PLM_MODEL_PATH} \\\n",
    "    --tokenizer-kwargs padding_side=\"left\" truncation=\"left\" \\\n",
    "    --max-seq-len 2048 \\\n",
    "    --batch-size 4 \\\n",
    "    --hf-num-gpus 2 \\\n",
    "    --work-dir \"/kaggle/working/evals/plm\" \\\n",
    "    --debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%opencompass \\\n",
    "    --datasets mmlu_ppl hellaswag_clean_ppl winogrande_ll ARC_e_ppl ARC_c_clean_ppl SuperGLUE_BoolQ_few_shot_ppl \\\n",
    "    --summarizer example \\\n",
    "    --hf-type base \\\n",
    "    --hf-path {SFT_MODEL_PATH} \\\n",
    "    --tokenizer-kwargs padding_side=\"left\" truncation=\"left\" \\\n",
    "    --max-seq-len 2048 \\\n",
    "    --batch-size 4 \\\n",
    "    --hf-num-gpus 2 \\\n",
    "    --work-dir \"/kaggle/working/evals/sft\" \\\n",
    "    --debug"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
